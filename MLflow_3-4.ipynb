{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noI0fVRSzx8f"
   },
   "source": [
    "**Module 3.4: MLflow with Docker & Cloud** \n",
    "## 🎯 **Learning Objectives Expanded**\n",
    "\n",
    "### 1️⃣ **Log an MLflow Model to Disk**\n",
    "\n",
    "* **What it means:**\n",
    "  Saving a trained ML model in a structured format on your disk so MLflow can manage it.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Train your ML model (e.g., Random Forest, Logistic Regression).\n",
    "  * Use MLflow's built-in logging methods, such as:\n",
    "\n",
    "    ```python\n",
    "    import mlflow.sklearn\n",
    "    mlflow.sklearn.log_model(model, \"model_name\")\n",
    "    ```\n",
    "  * MLflow creates a directory structure on disk, storing metadata, artifacts, and the model itself for later deployment or serving.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Ensures consistency, reproducibility, and easy management of models for deployment or future use.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **Build a Docker Image using `mlflow models build-docker`**\n",
    "\n",
    "* **What it means:**\n",
    "  Packaging the MLflow-logged model and its required environment into a Docker image. This allows the model to run in isolation with all necessary dependencies.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * After logging your ML model, use the command:\n",
    "\n",
    "    ```bash\n",
    "    mlflow models build-docker -m runs:/<RUN_ID>/model_name -n my_model_image\n",
    "    ```\n",
    "  * This command automatically generates a Docker image that includes the MLflow environment, your model, and necessary libraries.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Dockerizing your MLflow model ensures portability, consistency, and ease of deployment to different environments (e.g., cloud, local machines, Kubernetes).\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ **Serve the Image via Docker Container**\n",
    "\n",
    "* **What it means:**\n",
    "  Running the Docker image you've built as a container, effectively creating a RESTful API endpoint to serve model predictions.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Use the following Docker command:\n",
    "\n",
    "    ```bash\n",
    "    docker run -p 5001:8080 my_model_image\n",
    "    ```\n",
    "  * The Docker container exposes your model at port `8080` inside the container, which you map to `5001` on your local machine.\n",
    "  * Your model is now running as an API server and ready to accept prediction requests.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Serving your model in a Docker container isolates it from your local environment, making deployment safer, cleaner, and more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ **Test the Model using Curl or REST API**\n",
    "\n",
    "* **What it means:**\n",
    "  Sending data to your model's REST API endpoint and checking if it returns predictions correctly.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Once your Docker container is running, send a request like this:\n",
    "\n",
    "    ```bash\n",
    "    curl http://127.0.0.1:5001/invocations \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '[{\"feature1\": 5.1, \"feature2\": 3.5}]'\n",
    "    ```\n",
    "  * Alternatively, use Python's `requests` library:\n",
    "\n",
    "    ```python\n",
    "    import requests\n",
    "    url = \"http://127.0.0.1:5001/invocations\"\n",
    "    data = [{\"feature1\": 5.1, \"feature2\": 3.5}]\n",
    "    response = requests.post(url, json=data)\n",
    "    print(response.json())\n",
    "    ```\n",
    "\n",
    "* **Why it matters:**\n",
    "  Testing ensures your model correctly receives and processes input data, making accurate predictions before you integrate it into larger systems or workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24965,
     "status": "ok",
     "timestamp": 1753903543236,
     "user": {
      "displayName": "Rayan Yassminh",
      "userId": "13160170557035890181"
     },
     "user_tz": -60
    },
    "id": "1d1OKzXdG1_5",
    "outputId": "5c84be8e-0635-4ff8-e4a8-e4c9013475b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.0/677.0 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 19:25:38 INFO mlflow.tracking.fluent: Experiment with name 'dockerized-ml-model' does not exist. Creating a new experiment.\n",
      "2025/07/30 19:25:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/07/30 19:25:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model logged with Run ID: cae3a564eb5b406a8e9fd7bd71fd7736\n",
      "\n",
      "🐳 To build a Docker image for your model, use:\n",
      "\n",
      "mlflow models build-docker -m runs:/<RUN_ID>/rf_model -n my_mlflow_rf_image\n",
      "\n",
      "Then run the container locally with:\n",
      "\n",
      "docker run -p 5001:8080 my_mlflow_rf_image\n",
      "\n",
      "Make sure Docker is installed and running.\n",
      "\n",
      "\n",
      "📤 To send a test request:\n",
      "\n",
      "curl http://127.0.0.1:5001/invocations   -H \"Content-Type: application/json\"   -d '[{\"sepal length (cm)\": 5.1, \"sepal width (cm)\": 3.5, \"petal length (cm)\": 1.4, \"petal width (cm)\": 0.2}]'\n",
      "\n",
      "Replace the JSON with the correct feature names from your model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 📓 Module 3.4: MLflow with Docker & Cloud\n",
    "# Goal: Build a Docker image for a logged MLflow model and run it in a container or cloud\n",
    "\n",
    "# ✅ Step 1: Log a model to be packaged into a Docker image\n",
    "!pip install -q mlflow scikit-learn\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mlflow.set_experiment(\"dockerized-ml-model\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(model, \"rf_model\")\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(\"✅ Model logged with Run ID:\", run_id)\n",
    "\n",
    "# ✅ Step 2: Export command to build Docker image (run in terminal)\n",
    "print(\"\"\"\n",
    "🐳 To build a Docker image for your model, use:\n",
    "\n",
    "mlflow models build-docker -m runs:/<RUN_ID>/rf_model -n my_mlflow_rf_image\n",
    "\n",
    "Then run the container locally with:\n",
    "\n",
    "docker run -p 5001:8080 my_mlflow_rf_image\n",
    "\n",
    "Make sure Docker is installed and running.\n",
    "\"\"\")\n",
    "\n",
    "# ✅ Step 3: Sample curl command to test the Dockerized model (terminal only)\n",
    "print(\"\"\"\n",
    "📤 To send a test request:\n",
    "\n",
    "curl http://127.0.0.1:5001/invocations \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '[{\"sepal length (cm)\": 5.1, \"sepal width (cm)\": 3.5, \"petal length (cm)\": 1.4, \"petal width (cm)\": 0.2}]'\n",
    "\n",
    "Replace the JSON with the correct feature names from your model.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJi2GQKqPfDK"
   },
   "source": [
    "---\n",
    "\n",
    "## 📝 Assessment: MLflow with Docker & Cloud    \n",
    "\n",
    "### 📘 Multiple Choice (Correct answers in **bold**)    \n",
    "\n",
    "**1. What does `mlflow models build-docker` do?**    \n",
    "A. Starts a Docker container    \n",
    "**B. Creates a Docker image that can serve your MLflow model** ✅    \n",
    "C. Logs a model to S3    \n",
    "D. Registers a model with the cloud registry    \n",
    "\n",
    "---\n",
    "\n",
    "**2. Which default port does a Dockerized MLflow model serve on?**    \n",
    "A. 8081    \n",
    "**B. 8080** ✅    \n",
    "C. 5000    \n",
    "D. 6006    \n",
    "\n",
    "---\n",
    "\n",
    "**3. What must be installed on your system to run a Dockerized MLflow model?**    \n",
    "A. Anaconda    \n",
    "**B. Docker engine** ✅    \n",
    "C. Kubernetes    \n",
    "D. Apache Spark    \n",
    "\n",
    "---\n",
    "\n",
    "**4. Which MLflow flavor(s) support Docker-based serving?**    \n",
    "A. Only pyfunc    \n",
    "**B. All flavors via pyfunc abstraction** ✅    \n",
    "C. Only sklearn    \n",
    "D. Only models from the registry    \n",
    "\n",
    "---\n",
    "\n",
    "### ✏️ Short Answer\n",
    "\n",
    "**5. What are the benefits of containerizing MLflow models using Docker?**    \n",
    "*Ensures environment consistency, simplifies deployment, allows scaling in production environments, and removes dependency issues.*    \n",
    "\n",
    "---\n",
    "\n",
    "**6. Why is it important to expose the port (e.g., `-p 5001:8080`) when running a Docker container?**    \n",
    "*It maps the container's internal port (8080) to a host machine port (5001),     making the REST API accessible from outside the container.*    \n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Mini Project\n",
    "\n",
    "**7. Task:**    \n",
    "\n",
    "* Log any MLflow-compatible model    \n",
    "* Build a Docker image using `mlflow models build-docker`    \n",
    "* Run the container locally with Docker    \n",
    "* Use `curl` or `requests.post()` to send JSON input and receive prediction    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNu5WwvOdbJO2n6Or9dTkRk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
