{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKWYxNL1qP5z"
   },
   "source": [
    " **Module 1.5: Organizing Experiments in MLflow**\n",
    "\n",
    "\n",
    "## üéØ **Learning Objectives Expanded**\n",
    "\n",
    "### 1Ô∏è‚É£ **Create and Name Experiments Using `mlflow.set_experiment()`**\n",
    "\n",
    "* **What it means:**\n",
    "  Organizing your MLflow runs by assigning them to clearly named experiments, making it easier to group related runs logically.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Set the experiment name:\n",
    "\n",
    "    ```python\n",
    "    import mlflow\n",
    "    mlflow.set_experiment(\"model-selection-experiment\")\n",
    "    ```\n",
    "  * If the experiment doesn‚Äôt exist, MLflow automatically creates it.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Clear experiment naming helps you easily manage, identify, and compare related model runs over time.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Use Tags to Track Metadata (e.g., stage, author, purpose)**\n",
    "\n",
    "* **What it means:**\n",
    "  Adding custom labels or \"tags\" to your MLflow runs to capture extra details beyond just parameters and metrics (e.g., author, project phase).\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Define tags explicitly in your run:\n",
    "\n",
    "    ```python\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"stage\", \"development\")\n",
    "        mlflow.set_tag(\"author\", \"John Doe\")\n",
    "        mlflow.set_tag(\"purpose\", \"Baseline model test\")\n",
    "    ```\n",
    "\n",
    "* **Why it matters:**\n",
    "  Tags enhance your ability to filter, search, and organize experiment runs based on specific metadata or context, improving team collaboration and understanding.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Log and Organize Multiple Model Types Within a Single Experiment**\n",
    "\n",
    "* **What it means:**\n",
    "  Using a single named experiment to manage runs of various model types (e.g., Linear Regression, Random Forest, XGBoost), simplifying comparison and management.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  ```python\n",
    "  models = [\"linear_regression\", \"random_forest\", \"xgboost\"]\n",
    "  for model_type in models:\n",
    "      with mlflow.start_run():\n",
    "          mlflow.log_param(\"model_type\", model_type)\n",
    "          mlflow.set_tag(\"stage\", \"baseline\")\n",
    "\n",
    "          # Train and log different model types here\n",
    "          mlflow.log_metric(\"accuracy\", accuracy)\n",
    "  ```\n",
    "\n",
    "* **Why it matters:**\n",
    "  Centralizing diverse model runs within one experiment allows easy comparative analysis, enabling better model selection decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **View Structured Experiment Data Using `search_runs()`**\n",
    "\n",
    "* **What it means:**\n",
    "  Retrieving and reviewing experiment results (parameters, metrics, tags, etc.) programmatically using MLflow‚Äôs Python API.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Retrieve structured data as a DataFrame:\n",
    "\n",
    "    ```python\n",
    "    runs_df = mlflow.search_runs(experiment_names=[\"model-selection-experiment\"])\n",
    "    runs_df[[\"run_id\", \"params.model_type\", \"metrics.accuracy\", \"tags.stage\"]]\n",
    "    ```\n",
    "  * Sort and analyze runs:\n",
    "\n",
    "    ```python\n",
    "    runs_df.sort_values(by=\"metrics.accuracy\", ascending=False)\n",
    "    ```\n",
    "\n",
    "* **Why it matters:**\n",
    "  Quick access to structured experiment data helps in rapidly comparing results, identifying top-performing models, and understanding experimental outcomes in detail.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iNKpGrqCqRzv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/02 22:02:37 INFO mlflow.tracking.fluent: Experiment with name 'model-selection-experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 'model-selection-experiment' created with ID: 222412334690476895\n",
      "Logged run for model: linear_regression, Run ID: 921812ecce1f45b0b5e49a107d27bba1\n",
      "Logged run for model: random_forest, Run ID: 8e9c8f88cd30491795a0eb61957cce20\n",
      "\n",
      "All runs for the experiment:\n",
      "                             run_id  params.model_type  metrics.accuracy  \\\n",
      "0  8e9c8f88cd30491795a0eb61957cce20      random_forest               0.9   \n",
      "1  921812ecce1f45b0b5e49a107d27bba1  linear_regression               0.8   \n",
      "\n",
      "  tags.stage  \n",
      "0        dev  \n",
      "1        dev  \n"
     ]
    }
   ],
   "source": [
    "# üìì Module 1.5: Organizing Experiments in MLflow\n",
    "# Goal: Understand how to create, retrieve, and use named experiments effectively in MLflow.\n",
    "\n",
    "# ‚úÖ Step 1: Install MLflow\n",
    "!pip install -q mlflow\n",
    "\n",
    "# ‚úÖ Step 2: Import necessary libraries\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "# ‚úÖ Step 3: Create or set an experiment\n",
    "# This experiment name will group all runs under a single label for easy comparison\n",
    "experiment_name = \"model-selection-experiment\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Retrieve the experiment to confirm creation or access\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experiment '{experiment.name}' created with ID: {experiment.experiment_id}\")\n",
    "\n",
    "# ‚úÖ Step 4: Log dummy runs under this experiment\n",
    "# Demonstrates how to use tags and structure runs clearly\n",
    "for model_type in [\"linear_regression\", \"random_forest\"]:\n",
    "    with mlflow.start_run():\n",
    "        # Log a dummy model type as a parameter\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "\n",
    "        # Use tags to add metadata (e.g., user, purpose)\n",
    "        mlflow.set_tag(\"stage\", \"dev\")\n",
    "        mlflow.set_tag(\"author\", \"mlflow_course\")\n",
    "\n",
    "        # Log fake metric for demo purposes\n",
    "        mlflow.log_metric(\"accuracy\", 0.8 if model_type == \"linear_regression\" else 0.9)\n",
    "\n",
    "        print(f\"Logged run for model: {model_type}, Run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "# ‚úÖ Step 5: Retrieve and inspect all runs in the experiment\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "print(\"\\nAll runs for the experiment:\")\n",
    "display_cols = [\"run_id\", \"params.model_type\", \"metrics.accuracy\", \"tags.stage\"]\n",
    "print(runs_df[display_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJC1fFe6qiOg"
   },
   "source": [
    "## üìù Assessment: Organizing Experiments\n",
    "\n",
    "### üìò Multiple Choice (Choose the best answer)\n",
    "\n",
    "**1. What does `mlflow.set_experiment(\"my-exp\")` do?**   \n",
    "A. Starts a new run with the given name   \n",
    "**B. Sets or creates an experiment to group related runs** ‚úÖ   \n",
    "C. Tags a model as experimental   \n",
    "D. Initializes the experiment UIv\n",
    "\n",
    "---\n",
    "\n",
    "**2. Why would you use `mlflow.set_tag()`?**   \n",
    "A. To assign a unique ID to a model   \n",
    "B. To deploy a model to production\n",
    "**C. To attach metadata (e.g., author, stage) to a run** ‚úÖ   \n",
    "D. To search for artifacts   \n",
    "\n",
    "---\n",
    "\n",
    "**3. What is the correct method to retrieve all runs from a specific experiment?**   \n",
    "A. `mlflow.get_runs()`   \n",
    "**B. `mlflow.search_runs()`** ‚úÖ   \n",
    "C. `mlflow.find_experiment()`   \n",
    "D. `mlflow.list_models()`   \n",
    "\n",
    "---\n",
    "\n",
    "**4. What happens if you call `mlflow.set_experiment()` with a name that doesn‚Äôt exist yet?**   \n",
    "A. It raises an error   \n",
    "**B. It automatically creates the new experiment** ‚úÖ   \n",
    "C. It uses a default experiment   \n",
    "D. It disables tracking   \n",
    "\n",
    "---\n",
    "   \n",
    "### ‚úèÔ∏è Short Answer\n",
    "v\n",
    "**5. Why is it helpful to use tags when logging MLflow runs?**   \n",
    "*Tags help identify purpose, environment (dev/prod), authorship, or versioning for better tracking and collaboration.*   \n",
    "\n",
    "---\n",
    "\n",
    "**6. What‚Äôs the difference between an experiment and a run in MLflow?**   \n",
    "*An experiment is a container for grouping multiple runs; each run represents one model training execution.*   \n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Mini Project   \n",
    "\n",
    "**7. Task:**   \n",
    "You‚Äôre comparing 3 models: Linear Regression, Decision Tree, and XGBoost.v\n",
    "\n",
    "* Use `mlflow.set_experiment(\"model-comparison\")`   \n",
    "* For each model:   \n",
    "\n",
    "  * Log the model type as a parameter   \n",
    "  * Set a `stage` tag (e.g., ‚Äúbaseline‚Äù)   \n",
    "  * Log an example metric (e.g., accuracy or RMSE)   \n",
    "* Retrieve all runs using `search_runs()`   \n",
    "* Export a table with columns: run ID, model type, metric, stage   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGXWsj3-qg-R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNlp/xQs4B1BGYEakczAQeG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
