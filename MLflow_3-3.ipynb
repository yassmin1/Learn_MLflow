{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buq0cOhDF-6F"
   },
   "source": [
    "âœ… **Module 3.3: Serving a Model via REST API** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRSpS2IDrTDj"
   },
   "source": [
    "## ğŸ¯ **Learning Objectives**\n",
    "\n",
    "### 1ï¸âƒ£ **Log a Model with MLflow for Serving**\n",
    "\n",
    "* **What it means:**\n",
    "  Logging a model with MLflow means saving your trained model in a standardized format, making it ready to deploy or share with others.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Train your machine learning model (e.g., Linear Regression, Random Forest, etc.).\n",
    "  * Use MLflow functions like `mlflow.sklearn.log_model()` to store the model, parameters, and metadata.\n",
    "  * MLflow stores this information so it can later serve the model or load it for predictions.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Logging ensures your model is reproducible, organized, versioned, and easily retrievable for serving.\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Serve the Model Using `mlflow models serve`**\n",
    "\n",
    "* **What it means:**\n",
    "  Serving means deploying your logged model on a server, turning it into a RESTful API that accepts prediction requests and sends back predictions.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * After logging the model, run the following command in your terminal:\n",
    "\n",
    "    ```bash\n",
    "    mlflow models serve -m runs:/<RUN_ID>/model_name -p 5001 --no-conda\n",
    "    ```\n",
    "  * `runs:/<RUN_ID>/model_name` specifies the model you want to serve.\n",
    "  * `-p 5001` indicates the port number your API listens to.\n",
    "  * `--no-conda` means it uses the current Python environment instead of creating a new Conda environment.\n",
    "\n",
    "* **Why it matters:**\n",
    "  Serving models through a REST API allows easy integration with web apps, mobile apps, and other software, enabling real-time predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Send REST API Requests with JSON Payload**\n",
    "\n",
    "* **What it means:**\n",
    "  Once your model is served, it expects data for predictions. REST API requests are how you send this data (input features) to your model.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * Create input data as JSON (usually from a pandas DataFrame or Python dictionary):\n",
    "\n",
    "    ```python\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    url = \"http://127.0.0.1:5001/invocations\"\n",
    "    data = pd.DataFrame({\"feature1\": [0.5], \"feature2\": [1.5]}).to_json(orient=\"records\")\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    print(response.json())\n",
    "    ```\n",
    "  * Use `requests.post()` to send the data to the model endpoint.\n",
    "\n",
    "* **Why it matters:**\n",
    "  JSON payloads are a universal way to send structured data over the web. Using JSON ensures your API can easily communicate with various software systems.\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **Interpret API Response from `/invocations` Endpoint**\n",
    "\n",
    "* **What it means:**\n",
    "  The `/invocations` endpoint is where the REST API serves predictions. The response usually includes predictions made by the model based on the input provided.\n",
    "\n",
    "* **Detailed Steps:**\n",
    "\n",
    "  * When you send data to `/invocations`, the API returns predictions as JSON. For example:\n",
    "\n",
    "    ```json\n",
    "    [20.3]\n",
    "    ```\n",
    "\n",
    "    Or for classification:\n",
    "\n",
    "    ```json\n",
    "    {\"predicted_label\": 1, \"probability\": 0.85}\n",
    "    ```\n",
    "  * Your application interprets this response (e.g., showing predictions to users or making automated decisions).\n",
    "\n",
    "* **Why it matters:**\n",
    "  Clearly interpreting API responses allows your applications or workflows to make accurate and informed decisions based on real-time model predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19337,
     "status": "ok",
     "timestamp": 1753902192452,
     "user": {
      "displayName": "Rayan Yassminh",
      "userId": "13160170557035890181"
     },
     "user_tz": -60
    },
    "id": "Pzl3nGNSFy_v",
    "outputId": "e33aa078-3c9a-4868-bc4a-a63f17d450b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m677.0/677.0 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 19:03:08 INFO mlflow.tracking.fluent: Experiment with name 'serve-linear-model' does not exist. Creating a new experiment.\n",
      "2025/07/30 19:03:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/07/30 19:03:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged with run ID: e1f6ac60eb134a0aa9d31d74d0ed7ecb\n",
      "\n",
      "âš™ï¸ To serve the model, run this command in your terminal:\n",
      "\n",
      "mlflow models serve -m runs:/<RUN_ID>/linear_model -p 5001 --no-conda\n",
      "\n",
      "Replace <RUN_ID> with the actual run_id printed above.\n",
      "\n",
      "Then open another terminal or Python script to send a request.\n",
      "\n",
      "\n",
      "ğŸ“¤ Example request (run separately in Python or terminal):\n",
      "\n",
      "import requests\n",
      "import pandas as pd\n",
      "\n",
      "url = \"http://127.0.0.1:5001/invocations\"\n",
      "data = pd.DataFrame({\"feature1\": [0.5], \"feature2\": [1.5]}).to_json(orient=\"records\")\n",
      "headers = {\"Content-Type\": \"application/json\"}\n",
      "\n",
      "response = requests.post(url, data=data, headers=headers)\n",
      "print(\"Prediction:\", response.json())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ““ Module 3.3: Serving a Model via REST API\n",
    "# Goal: Serve MLflow models as REST endpoints and test them with requests\n",
    "\n",
    "# âš ï¸ This notebook shows the steps but serving only works locally or on cloud VMs (not in Colab)\n",
    "\n",
    "# âœ… Step 1: Train and log a model to be served\n",
    "!pip install -q mlflow scikit-learn pandas\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate data\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n",
    "X_df = pd.DataFrame(X, columns=[\"feature1\", \"feature2\"])\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Log model\n",
    "mlflow.set_experiment(\"serve-linear-model\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(model, \"linear_model\")\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(\"Model logged with run ID:\", run_id)\n",
    "\n",
    "# âœ… Step 2: Serve the model (run in terminal or CLI)\n",
    "print(\"\"\"\n",
    "âš™ï¸ To serve the model, run this command in your terminal:\n",
    "\n",
    "mlflow models serve -m runs:/<RUN_ID>/linear_model -p 5001 --no-conda\n",
    "\n",
    "Replace <RUN_ID> with the actual run_id printed above.\n",
    "\n",
    "Then open another terminal or Python script to send a request.\n",
    "\"\"\")\n",
    "\n",
    "# âœ… Step 3: Send a request to the REST endpoint\n",
    "print(\"\"\"\n",
    "ğŸ“¤ Example request (run separately in Python or terminal):\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = \"http://127.0.0.1:5001/invocations\"\n",
    "data = pd.DataFrame({\"feature1\": [0.5], \"feature2\": [1.5]}).to_json(orient=\"records\")\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, data=data, headers=headers)\n",
    "print(\"Prediction:\", response.json())\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCjZXXvuPqNL"
   },
   "source": [
    "#ğŸ“ Assessment: Serving a Model via REST API\n",
    "\n",
    "### ğŸ“˜ Multiple Choice (Correct answers in **bold**)    \n",
    "\n",
    "**1. What is the purpose of the MLflow command `mlflow models serve`?**    \n",
    "A. Register a model in the model registry    \n",
    "**B. Start a REST server for the model endpoint** âœ…    \n",
    "C. Launch the MLflow UI\n",
    "D. Trigger a batch training job\n",
    "\n",
    "---\n",
    "\n",
    "**2. Which endpoint does MLflow use for model inference by default?**   \n",
    "A. `/predict`   \n",
    "**B. `/invocations`** âœ…   \n",
    "C. `/model/serve`   \n",
    "D. `/run`   \n",
    "\n",
    "---\n",
    "\n",
    "**3. Which content type must you specify in the REST request header when sending a pandas DataFrame in JSON?**   \n",
    "A. `application/csv`   \n",
    "**B. `application/json`** âœ…   \n",
    "C. `text/plain`   \n",
    "D. `application/x-www-form-urlencoded`   \n",
    "\n",
    "---\n",
    "\n",
    "**4. What is the purpose of the `--no-conda` flag in the `mlflow models serve` command?**   \n",
    "A. Disables environment logging   \n",
    "**B. Prevents MLflow from creating a new Conda environment** âœ…   \n",
    "C. Skips model versioning   \n",
    "D. Serves the model using Docker   \n",
    "\n",
    "---\n",
    "\n",
    "### âœï¸ Short Answer   \n",
    "\n",
    "**5. Why might you choose to serve your model via REST API instead of batch prediction?**   \n",
    "*To enable real-time, on-demand inference where clients (e.g., web apps or microservices) can query the model interactively.*   \n",
    "\n",
    "---\n",
    "\n",
    "**6. What could cause a REST request to fail when serving a model? List two common reasons.**   \n",
    "\n",
    "* The server is not running on the correct port   \n",
    "* The input JSON is improperly formatted or missing required columns   \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Mini Project\n",
    "\n",
    "**7. Task:**   \n",
    "\n",
    "* Log a simple classification model (e.g., logistic regression)   \n",
    "* Serve it locally with `mlflow models serve`   \n",
    "* Use Pythonâ€™s `requests` module to POST a test sample in JSON format   \n",
    "* Display the modelâ€™s prediction response   \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6Bi5DT4pG07dj8beSJXYO",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
